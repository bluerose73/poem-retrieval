{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searcher类实现搜索算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Searcher(object):\n",
    "    def __init__(self):\n",
    "        with open('./var/poem_list', 'rb') as f:\n",
    "            self.poem_list = pickle.load(f)\n",
    "        # 按诗人的倒排索引\n",
    "        with open('./var/poet2poem', 'rb') as f:\n",
    "            self.poet2poem = pickle.load(f)\n",
    "        self.poem_num = len(self.poem_list)\n",
    "        self.results = []\n",
    "        self.keywords = []\n",
    "        self.TF_IDF = np.load('./var/TF_IDF.npz')['arr']\n",
    "        with open('./var/word2id', 'rb') as f:\n",
    "            self.word2id = pickle.load(f)\n",
    "        with open('./var/id2word', 'rb') as f:\n",
    "            self.id2word = pickle.load(f)\n",
    "        with open('./var/synonym_list', 'rb') as f:\n",
    "            self.synonym_list = pickle.load(f)\n",
    "        with open('./var/big2id', 'rb') as f:\n",
    "            self.big2id = pickle.load(f)\n",
    "        with open('./var/id2big', 'rb') as f:\n",
    "            self.id2big = pickle.load(f)\n",
    "    \n",
    "    def new_search(self, poet, keywords):\n",
    "        '''\n",
    "        发起新的搜索\n",
    "        '''\n",
    "        # 根据诗人搜素\n",
    "        poet_lim = []\n",
    "        if poet == '':\n",
    "            poet_lim = list(range(self.poem_num))\n",
    "        elif poet not in self.poet2poem:\n",
    "            self.results.append([])\n",
    "            self.keywords.append([])\n",
    "            return\n",
    "        else:\n",
    "            poet_lim = []\n",
    "            for i in self.poet2poem[poet]:\n",
    "                poet_lim.append(i)\n",
    "        poet_dict = {}\n",
    "        for i in poet_lim:\n",
    "            poet_dict[i] = 1\n",
    "\n",
    "        # 根据关键词搜索\n",
    "        if keywords.split() == []:\n",
    "            res = []\n",
    "            for i in poet_lim:\n",
    "                res.append(self.poem_list[i])\n",
    "            self.results.append(res)\n",
    "            self.keywords.append([])\n",
    "            return\n",
    "            \n",
    "        # 为每一首诗打分\n",
    "        kw = keywords.split()\n",
    "        append_kw = kw.copy()\n",
    "        \n",
    "        score = np.zeros(self.poem_num)\n",
    "        for word in kw:\n",
    "            word_id = self.word2id[word]\n",
    "            score += self.TF_IDF[:, word_id] * 1000\n",
    "            if word_id in self.id2big:\n",
    "                word_big = self.id2big[word_id]\n",
    "                factor = 1\n",
    "                for syn_big in self.synonym_list[word_big]:\n",
    "                    syn_id = self.big2id[syn_big]\n",
    "                    append_kw.append(self.id2word[syn_id])\n",
    "                    score += self.TF_IDF[:, syn_id] * factor\n",
    "                    factor /= 2\n",
    "            \n",
    "        \n",
    "        res_ind = np.argsort(-score)\n",
    "        res_ind = res_ind[score[res_ind] > 1e-6]\n",
    "        res_ind = res_ind[[(x in poet_dict) for x in res_ind]]\n",
    "        \n",
    "        if res_ind.shape[0] <= 0:\n",
    "            self.results.append([])\n",
    "            self.keywords.append(append_kw)\n",
    "            return\n",
    "        \n",
    "        #print('result number:', len(res_ind))\n",
    "        if res_ind.shape[0] > 100:\n",
    "            res_ind = res_ind[:100]\n",
    "        \n",
    "        # HITS\n",
    "        edge = self.make_edge(res_ind, self.TF_IDF)\n",
    "        aut = self.HITS(edge)\n",
    "        \n",
    "        pairs = sorted(list(zip(aut, res_ind)))\n",
    "        pairs.reverse()\n",
    "        res_ind = [i[1] for i in pairs]\n",
    "        \n",
    "        # 重整，含有 keyword 的提上去\n",
    "        res_kw = []\n",
    "        res_syn = []\n",
    "        for poem in res_ind:\n",
    "            has_kw = False\n",
    "            for word in kw:\n",
    "                if self.TF_IDF[poem, self.word2id[word]] > 0:\n",
    "                    has_kw = True\n",
    "                    break\n",
    "            if has_kw:\n",
    "                res_kw.append(poem)\n",
    "            else:\n",
    "                res_syn.append(poem)\n",
    "        res_ind = res_kw + res_syn\n",
    "        \n",
    "        res = []\n",
    "        for i in res_ind:\n",
    "            res.append(self.poem_list[i])\n",
    "        self.results.append(res)\n",
    "        self.keywords.append(append_kw)\n",
    "            \n",
    "    def make_edge(self, search_res, TF_IDF):\n",
    "        '''\n",
    "        根据搜索结果建超链接图\n",
    "        '''\n",
    "        search_res = np.sort(search_res)\n",
    "\n",
    "        feature = TF_IDF[search_res]\n",
    "        #print(search_res)\n",
    "\n",
    "        size = len(search_res)\n",
    "\n",
    "        from sklearn.decomposition import PCA\n",
    "        dec_to = min(32, feature.shape[0], feature.shape[1])\n",
    "        feature = PCA(n_components = dec_to).fit_transform(feature)\n",
    "\n",
    "        edge = np.zeros((size, size))\n",
    "\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                edge[i][j] = edge[j][i] = np.linalg.norm(feature[i]-feature[j])\n",
    "\n",
    "        # 正常的网页中，超链接数和网页数应当是同阶的\n",
    "        # 这里，就选最大的 5n 条边\n",
    "        threshold = np.sort(edge.flatten())[min(5 * size, size * size - 1)]\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                edge[i, j] = 1 if edge[i, j] < threshold else 0\n",
    "        return edge\n",
    "    \n",
    "    def HITS(self, edge):\n",
    "        '''\n",
    "        HITS 算法，读入代表超链接的图，\n",
    "        返回每个节点的权威值\n",
    "        '''\n",
    "        max_iteration = 200\n",
    "        error = 0.0001\n",
    "        size = edge.shape[0]\n",
    "\n",
    "        aut = np.ones(size).T\n",
    "        hub = np.ones(size).T\n",
    "\n",
    "        k = 0\n",
    "        while k < max_iteration:\n",
    "            err = 0\n",
    "            k += 1\n",
    "\n",
    "            new_aut = edge.T @ hub\n",
    "            new_hub = edge @ new_aut\n",
    "\n",
    "            new_aut /= np.linalg.norm(new_aut)\n",
    "            new_hub /= np.linalg.norm(new_hub)\n",
    "\n",
    "            if np.linalg.norm(new_aut - aut) < error:\n",
    "                break\n",
    "\n",
    "            aut = new_aut\n",
    "            hub = new_hub\n",
    "\n",
    "        return aut.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下是测试\n",
    "s = Searcher()\n",
    "s.new_search('李白', '西风')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['西风', '北风', '狂风', '凉风']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.keywords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'紫藤树\\n李白\\n紫藤挂云木\\n花蔓宜阳春\\n密叶隐歌鸟\\n香风留美人\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.poem_list[372]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'春怨\\n李白\\n白马金羁辽海东\\n罗帷绣被卧春风\\n落月低轩窥烛尽\\n飞花入户笑床空\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.poem_list[381]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2059"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.word2id['春']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
